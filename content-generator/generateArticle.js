const searchOnGoogle = require("./googleSearch");
const scrapeArticle = require("./scrapeContent");
const rewriteArticle = require("./aiRewrite");

async function generateNewArticle(title) {

  console.log("\nðŸ“ Processing Article:", title);

  const query = title.replace(/[:?]/g, "") + " chatbot blog article";

  const links = await searchOnGoogle(query);
  console.log("ðŸ”— Search Links:", links);

  if (!links || links.length === 0) {
    console.log("âš  No links found â€” skipping");
    return null;
  }

  const validLinks = links
    .filter(l => l && l.startsWith("http"))
    .slice(0, 3);

  if (validLinks.length < 2) {
    console.log("âš  Not enough valid links â€” skipping");
    return null;
  }

  // ðŸŸ¡ scrape top 2 links
  let c1 = await scrapeArticle(validLinks[0]);
  let c2 = await scrapeArticle(validLinks[1]);

  // ðŸ” fallback if one fails
  if (!c1 || !c2) {
    console.log("âš  One source failed â€” trying extra linkâ€¦");

    if (!c1 && validLinks[2]) c1 = await scrapeArticle(validLinks[2]);
    if (!c2 && validLinks[2]) c2 = await scrapeArticle(validLinks[2]);
  }

  if (!c1 || !c2) {
    console.log("â© Skipping â€” still missing content");
    return null;
  }

  // âœ‚ Limit raw scraped text (prevents token overload)
  const combined =
    (c1 + "\n\n" + c2).slice(0, 12000);

  // âœ AI Rewrite (with retry logic inside aiRewrite.js)
  const rewritten = await rewriteArticle(combined, validLinks);

  // ðŸš« Safety check â€” do NOT publish empty result
  if (!rewritten || rewritten.trim().length < 200) {
    console.log("âš  Rewrite failed or too short â€” skipping");
    return null;
  }

  return {
    title,
    content: rewritten,
    source1: validLinks[0],
    source2: validLinks[1]
  };
}

module.exports = generateNewArticle;